{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from utils.feature_extractor_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=30,\n",
    "        hidden_dim=60,\n",
    "        dense_dim=512,\n",
    "        output_dim=4,\n",
    "        num_layers=2,\n",
    "        use_gpu=False,\n",
    "        batch_size=1,\n",
    "        is_training=False,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ;input_dim: 30\n",
    "            ;hidden_dim: 60\n",
    "            ;dense_dim: 512\n",
    "            ;output_dim: 4\n",
    "            ;num_layers: 2 #stack two bilstm layers\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # inti self values\n",
    "        self.use_gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.is_training = True\n",
    "\n",
    "        # define layers\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dense_hidden = nn.Linear(hidden_dim * 2, dense_dim)\n",
    "        self.dense_out = nn.Linear(dense_dim, output_dim)\n",
    "\n",
    "        # define hidden, cell for BiLSTM\n",
    "        ## (num_layers * num_directions, batch, hidden_size)\n",
    "        if use_gpu:\n",
    "            self.h_0 = Variable(\n",
    "                torch.zeros(2 * num_layers, self.batch_size, hidden_dim).cuda()\n",
    "            )\n",
    "            self.c_0 = Variable(\n",
    "                torch.zeros(2 * num_layers, self.batch_size, hidden_dim).cuda()\n",
    "            )\n",
    "        else:\n",
    "            self.h_0 = Variable(\n",
    "                torch.zeros(2 * num_layers, self.batch_size, hidden_dim)\n",
    "            )\n",
    "            self.c_0 = Variable(\n",
    "                torch.zeros(2 * num_layers, self.batch_size, hidden_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, audio_features):\n",
    "        # audio_features = (seq_len, batch, input_size)\n",
    "        lstm_output, (h_1, c_1) = self.bilstm(audio_features)\n",
    "\n",
    "        # (seq_len, batch, input_size)  => (batch, input_size), only last output\n",
    "        hidden_1 = self.dense_hidden(lstm_output[-1])\n",
    "        y = self.dense_out(hidden_1)\n",
    "\n",
    "        # for cross entropy loss\n",
    "        if self.is_training:\n",
    "            return y\n",
    "        else:\n",
    "            return F.softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm = BiLSTM()\n",
    "bilstm.load_state_dict(torch.load(\"../data/bilstm_state_dict.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict wav file with pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'ang': 0, 'exc': 1, 'neu': 2, 'sad': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file_directory = \"../data/speech_sample.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label_of_wav(wav_file):\n",
    "    extracted_feature = feature_generator(wav_file)\n",
    "    extracted_feature = np.expand_dims(extracted_feature, 0) # batch_dim\n",
    "    feature_tensor = torch.tensor(extracted_feature).float().permute(2,0,1)\n",
    "    predicted_label = bilstm(feature_tensor).argmax(dim=1).cpu().numpy()[0]\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/irteam/anaconda3/envs/py36_torch/lib/python3.6/site-packages/librosa/filters.py:235: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label_of_wav(wav_file_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36_torch]",
   "language": "python",
   "name": "conda-env-py36_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
