{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "from utils.feature_extractor_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parms\n",
    "use_gpu = True\n",
    "state_dict_directory = \"../data/bilstm_augmented_state_dict.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=30,\n",
    "        hidden_dim=60,\n",
    "        dense_dim=512,\n",
    "        output_dim=4,\n",
    "        num_layers=2,\n",
    "        use_gpu=False,\n",
    "        batch_size=1,\n",
    "        is_training=False,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ;input_dim: 30\n",
    "            ;hidden_dim: 60\n",
    "            ;dense_dim: 512\n",
    "            ;output_dim: 4\n",
    "            ;num_layers: 2 #stack two bilstm layers\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # inti self values\n",
    "        self.use_gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.is_training = True\n",
    "\n",
    "        # define layers\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dense_hidden = nn.Sequential(nn.Linear(hidden_dim*2, dense_dim),\n",
    "                                         nn.ReLU(inplace=True))\n",
    "        self.dense_out = nn.Linear(dense_dim, output_dim)\n",
    "\n",
    "    def forward(self, audio_features):\n",
    "        # audio_features = (seq_len, batch, input_size)\n",
    "        lstm_output, (h_1, c_1) = self.bilstm(audio_features)\n",
    "\n",
    "        # (seq_len, batch, input_size)  => (batch, input_size), only last output\n",
    "        hidden_1 = self.dense_hidden(lstm_output[-1])\n",
    "        y = self.dense_out(hidden_1)\n",
    "\n",
    "        # for cross entropy loss\n",
    "        if self.is_training:\n",
    "            return y\n",
    "        else:\n",
    "            return F.softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "bilstm = BiLSTM()\n",
    "bilstm.load_state_dict(torch.load(state_dict_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict wav file with pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'ang': 0, 'exc': 1, 'neu': 2, 'sad': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gpu\n",
    "if use_gpu:\n",
    "    bilstm.cuda()\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label_of_wav(wav_file, use_gpu, device):\n",
    "    # get features and transform\n",
    "    extracted_feature = feature_generator(wav_file)\n",
    "    extracted_feature = np.expand_dims(extracted_feature, 0) # batch_dim\n",
    "    \n",
    "    # convert features array to tensor\n",
    "    if use_gpu:\n",
    "        feature_tensor = torch.tensor(extracted_feature).float().permute(2,0,1).to(device)\n",
    "    else:\n",
    "        feature_tensor = torch.tensor(extracted_feature).float().permute(2,0,1)\n",
    "        \n",
    "    # predict\n",
    "    ###predicted_label = bilstm(feature_tensor).argmax(dim=1).cpu().numpy()[0]\n",
    "    predicted_label = bilstm(feature_tensor).argmax(dim=1).item()\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 3\n",
      "0.6739509105682373\n"
     ]
    }
   ],
   "source": [
    "wav_file = \"../data/speech_sample.wav\"\n",
    "\n",
    "a = time.time()\n",
    "print(\"label:\", predict_label_of_wav(wav_file, use_gpu, device))\n",
    "b = time.time()\n",
    "\n",
    "print(b-a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36_torch]",
   "language": "python",
   "name": "conda-env-py36_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
