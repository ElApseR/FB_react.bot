{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM hyperparams\n",
    "input_dim = 30\n",
    "hidden_dim = 60\n",
    "dense_dim = 512\n",
    "output_dim = 4\n",
    "num_layers = 2\n",
    "is_cuda = True\n",
    "is_training = True\n",
    "batch_size = 32\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        dense_dim,\n",
    "        output_dim,\n",
    "        num_layers,\n",
    "        is_training=True,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Description\n",
    "            Build bilstm network. dropout is not in use but added just for in case of use.\n",
    "        Args:\n",
    "            ;input_dim: 30\n",
    "            ;hidden_dim: 60\n",
    "            ;dense_dim: 512\n",
    "            ;output_dim: 4\n",
    "            ;num_layers: 2 #stack two bilstm layers\n",
    "            ;dropout: 0.5 #currently not in use\n",
    "        \"\"\"\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        # init self values\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.is_training = is_training\n",
    "\n",
    "        # define layers\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dense_hidden = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, dense_dim), nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dense_out = nn.Linear(dense_dim, output_dim)\n",
    "\n",
    "    def forward(self, audio_features):\n",
    "        # audio_features = (seq_len, batch, input_size)\n",
    "        lstm_output, (h_1, c_1) = self.bilstm(audio_features)\n",
    "\n",
    "        # (seq_len, batch, input_size)  => (batch, input_size), only last output\n",
    "        hidden_1 = self.dense_hidden(lstm_output[-1])\n",
    "        y = self.dense_out(hidden_1)\n",
    "\n",
    "        # for cross entropy loss\n",
    "        if self.is_training:\n",
    "            return y\n",
    "        else:\n",
    "            return F.softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm = BiLSTM(\n",
    "    input_dim, hidden_dim, dense_dim, output_dim, num_layers, is_training, dropout\n",
    ")\n",
    "\n",
    "if is_cuda:\n",
    "    bilstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(bilstm.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "data_dir = \"../data/feature_array/\"\n",
    "features_dir = data_dir + \"audio_features.npy\"\n",
    "labels_dir = data_dir + \"labels.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "labels = np.load(labels_dir)\n",
    "features = np.load(features_dir)\n",
    "# features = np.moveaxis(features, 0, -2).transpose()\n",
    "\n",
    "# label string to onehot\n",
    "label_map_dict = {lab: i for i, lab in enumerate(np.unique(labels))}\n",
    "I_matrix = np.eye(len(label_map_dict))\n",
    "labels_onehot = np.array([I_matrix[label_map_dict[lab]] for lab in labels])\n",
    "labels_int = np.array([label_map_dict[lab] for lab in labels])\n",
    "\n",
    "print(label_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split: 대략 10%\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels_int, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, train_features, train_labels, is_cuda):\n",
    "        self.len = len(train_labels)\n",
    "        if is_cuda:\n",
    "            self.X = torch.from_numpy(train_features).float().to(device)\n",
    "            self.y = torch.from_numpy(train_labels).long().to(device)\n",
    "        else:\n",
    "            self.X = torch.from_numpy(train_features).float()\n",
    "            self.y = torch.from_numpy(train_labels).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batch\n",
    "dataset = AudioDataset(train_features, train_labels, is_cuda)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run training process\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # (batch, input_size, seq_len) => (seq_len, batch, input_size)\n",
    "        inputs, answers = data\n",
    "        inputs_permute = inputs.permute(2, 0, 1)\n",
    "\n",
    "        outputs = bilstm(inputs_permute)\n",
    "        loss = criterion(outputs, answers)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data.item()\n",
    "        losses.append(loss)\n",
    "\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm.is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train predict\n",
    "if is_cuda:\n",
    "    sample_train = torch.tensor(train_features).float().permute(2, 0, 1).to(device)\n",
    "else:\n",
    "    sample_train = torch.tensor(train_features).float().permute(2, 0, 1)\n",
    "\n",
    "predict_train = bilstm(sample_train).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "print(\"train accuracy:\", sum(predict_train == train_labels) / len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predict\n",
    "if is_cuda:\n",
    "    sample_test = torch.tensor(test_features).float().permute(2, 0, 1).to(device)\n",
    "else:\n",
    "    sample_test = torch.tensor(test_features).float().permute(2, 0, 1)\n",
    "\n",
    "predict_test = bilstm(sample_test).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "print(\"test accuracy:\", sum(predict_test == test_labels) / len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "(pd.Series(labels).value_counts() / len(labels)).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save as state_dict\n",
    "- https://tutorials.pytorch.kr/beginner/saving_loading_models.html#state-dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(bilstm.state_dict(), \"../data/bilstm_state_dict.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36_torch]",
   "language": "python",
   "name": "conda-env-py36_torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
